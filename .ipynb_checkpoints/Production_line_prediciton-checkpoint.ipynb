{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  # Import numpy\n",
    "\n",
    "# Load your dataset from a CSV file\n",
    "data = pd.read_csv('new_dataset_production.csv')\n",
    "\n",
    "# Replace non-numeric values (e.g., '-') with NaN\n",
    "data = data.replace('-', np.nan)\n",
    "\n",
    "# Define the columns you want to fill with 0\n",
    "columns_to_fill_with_0 = ['Total SMV', 'Avg Per Day', 'Actual Per Day', 'Defect Qty', 'Rejected Qty', 'Missing Qty']\n",
    "\n",
    "# Fill missing values in the specified columns with 0\n",
    "data[columns_to_fill_with_0] = data[columns_to_fill_with_0].fillna(0)\n",
    "\n",
    "# Save the filled DataFrame to a new CSV file\n",
    "data.to_csv('production_dataset_filled.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new dataset\n",
    "data = pd.read_csv('new_dataset.csv')\n",
    "\n",
    "# Display the first few rows of the dataset to get an overview\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Summary statistics of numeric columns\n",
    "print(\"Summary statistics of numeric columns:\")\n",
    "print(data.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Check the data types of each column\n",
    "print(\"Data types of each column:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# Frequency of each unique value in the 'Module' column\n",
    "print(\"Frequency of each unique value in the 'Module' column:\")\n",
    "print(data['Module'].value_counts())\n",
    "\n",
    "# Frequency of each unique value in the 'Product Type' column\n",
    "print(\"Frequency of each unique value in the 'Product Type' column:\")\n",
    "print(data['Product Type'].value_counts())\n",
    "\n",
    "# Correlation matrix to see the relationships between numeric variables\n",
    "correlation_matrix = data.corr()\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Data distribution of the 'Module' column\n",
    "import matplotlib.pyplot as plt\n",
    "data['Module'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Module Distribution\")\n",
    "plt.xlabel(\"Module\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Data distribution of the 'Product Type' column\n",
    "data['Product Type'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Product Type Distribution\")\n",
    "plt.xlabel(\"Product Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0DjtVQbdKYw",
    "outputId": "ea69cd75-fad8-4dd0-caaf-cccd5d5eb0f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9906759906759907\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('/content/new_dataset_filled.csv')\n",
    "\n",
    "# Replace non-numeric values (e.g., '-') with NaN\n",
    "data = data.replace('-', float('nan'))\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "data['Module'] = le.fit_transform(data['Module'])\n",
    "data['Product Type'] = le.fit_transform(data['Product Type'])\n",
    "\n",
    "# Replace missing values with the mean of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "columns_to_impute = ['Total SMV', 'Avg Per Day', 'Actual Per Day', 'Defect Qty', 'Reject Qty', 'Missing Qty']\n",
    "data[columns_to_impute] = imputer.fit_transform(data[columns_to_impute])\n",
    "\n",
    "# Define features and target variable\n",
    "X = data[['Module', 'Product Type', 'Total SMV', 'Avg Per Day', 'Actual Per Day', 'Defect Qty', 'Reject Qty', 'Missing Qty']]\n",
    "# Replace 'Your_Target_Column_Name' with the actual name of your target variable.\n",
    "y = data['Product Type']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkI8tIUDdvdE",
    "outputId": "3c188a9c-3c99-4f34-90d9-e30b6fd71e54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "27/27 [==============================] - 2s 17ms/step - loss: 3.0733 - accuracy: 0.1525 - val_loss: 3.0500 - val_accuracy: 0.1585\n",
      "Epoch 2/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 3.0093 - accuracy: 0.1577 - val_loss: 2.9494 - val_accuracy: 0.1585\n",
      "Epoch 3/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 2.8454 - accuracy: 0.1980 - val_loss: 2.7025 - val_accuracy: 0.2098\n",
      "Epoch 4/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 2.5730 - accuracy: 0.2401 - val_loss: 2.4684 - val_accuracy: 0.2984\n",
      "Epoch 5/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 2.3904 - accuracy: 0.3102 - val_loss: 2.3167 - val_accuracy: 0.3566\n",
      "Epoch 6/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.2816 - accuracy: 0.3370 - val_loss: 2.2177 - val_accuracy: 0.3869\n",
      "Epoch 7/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.2063 - accuracy: 0.3598 - val_loss: 2.1435 - val_accuracy: 0.3800\n",
      "Epoch 8/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 2.1408 - accuracy: 0.3557 - val_loss: 2.0778 - val_accuracy: 0.3800\n",
      "Epoch 9/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.0786 - accuracy: 0.3662 - val_loss: 2.0177 - val_accuracy: 0.3893\n",
      "Epoch 10/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 2.0296 - accuracy: 0.3797 - val_loss: 1.9788 - val_accuracy: 0.4009\n",
      "Epoch 11/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.9924 - accuracy: 0.3960 - val_loss: 1.9461 - val_accuracy: 0.4103\n",
      "Epoch 12/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.9603 - accuracy: 0.3984 - val_loss: 1.9208 - val_accuracy: 0.4126\n",
      "Epoch 13/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.9300 - accuracy: 0.3949 - val_loss: 1.8939 - val_accuracy: 0.4126\n",
      "Epoch 14/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.9039 - accuracy: 0.3995 - val_loss: 1.8690 - val_accuracy: 0.4126\n",
      "Epoch 15/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.8811 - accuracy: 0.3949 - val_loss: 1.8479 - val_accuracy: 0.4126\n",
      "Epoch 16/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.8619 - accuracy: 0.4083 - val_loss: 1.8302 - val_accuracy: 0.4312\n",
      "Epoch 17/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.8468 - accuracy: 0.4100 - val_loss: 1.8157 - val_accuracy: 0.4312\n",
      "Epoch 18/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.8347 - accuracy: 0.4100 - val_loss: 1.8033 - val_accuracy: 0.4312\n",
      "Epoch 19/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.8246 - accuracy: 0.4100 - val_loss: 1.7884 - val_accuracy: 0.4359\n",
      "Epoch 20/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.8148 - accuracy: 0.4136 - val_loss: 1.7761 - val_accuracy: 0.4359\n",
      "Epoch 21/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.8079 - accuracy: 0.4159 - val_loss: 1.7656 - val_accuracy: 0.4476\n",
      "Epoch 22/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.8005 - accuracy: 0.4176 - val_loss: 1.7580 - val_accuracy: 0.4476\n",
      "Epoch 23/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7950 - accuracy: 0.4194 - val_loss: 1.7499 - val_accuracy: 0.4476\n",
      "Epoch 24/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7910 - accuracy: 0.4171 - val_loss: 1.7441 - val_accuracy: 0.4476\n",
      "Epoch 25/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7850 - accuracy: 0.4153 - val_loss: 1.7387 - val_accuracy: 0.4476\n",
      "Epoch 26/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7841 - accuracy: 0.4194 - val_loss: 1.7338 - val_accuracy: 0.4476\n",
      "Epoch 27/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7766 - accuracy: 0.4194 - val_loss: 1.7264 - val_accuracy: 0.4476\n",
      "Epoch 28/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7736 - accuracy: 0.4194 - val_loss: 1.7234 - val_accuracy: 0.4476\n",
      "Epoch 29/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7699 - accuracy: 0.4194 - val_loss: 1.7191 - val_accuracy: 0.4476\n",
      "Epoch 30/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7670 - accuracy: 0.4194 - val_loss: 1.7136 - val_accuracy: 0.4476\n",
      "Epoch 31/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7646 - accuracy: 0.4194 - val_loss: 1.7113 - val_accuracy: 0.4476\n",
      "Epoch 32/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7620 - accuracy: 0.4194 - val_loss: 1.7085 - val_accuracy: 0.4476\n",
      "Epoch 33/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7586 - accuracy: 0.4176 - val_loss: 1.7066 - val_accuracy: 0.4476\n",
      "Epoch 34/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7567 - accuracy: 0.4153 - val_loss: 1.7035 - val_accuracy: 0.4476\n",
      "Epoch 35/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7547 - accuracy: 0.4200 - val_loss: 1.7007 - val_accuracy: 0.4476\n",
      "Epoch 36/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7535 - accuracy: 0.4141 - val_loss: 1.6947 - val_accuracy: 0.4476\n",
      "Epoch 37/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7512 - accuracy: 0.4200 - val_loss: 1.6950 - val_accuracy: 0.4476\n",
      "Epoch 38/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7506 - accuracy: 0.4200 - val_loss: 1.6963 - val_accuracy: 0.4476\n",
      "Epoch 39/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7489 - accuracy: 0.4200 - val_loss: 1.6886 - val_accuracy: 0.4476\n",
      "Epoch 40/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7475 - accuracy: 0.4200 - val_loss: 1.6863 - val_accuracy: 0.4476\n",
      "Epoch 41/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7461 - accuracy: 0.4159 - val_loss: 1.6896 - val_accuracy: 0.4476\n",
      "Epoch 42/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7450 - accuracy: 0.4200 - val_loss: 1.6857 - val_accuracy: 0.4476\n",
      "Epoch 43/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7448 - accuracy: 0.4200 - val_loss: 1.6883 - val_accuracy: 0.4476\n",
      "Epoch 44/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7423 - accuracy: 0.4200 - val_loss: 1.6861 - val_accuracy: 0.4476\n",
      "Epoch 45/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7408 - accuracy: 0.4200 - val_loss: 1.6822 - val_accuracy: 0.4476\n",
      "Epoch 46/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7408 - accuracy: 0.4171 - val_loss: 1.6807 - val_accuracy: 0.4476\n",
      "Epoch 47/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7400 - accuracy: 0.4200 - val_loss: 1.6791 - val_accuracy: 0.4476\n",
      "Epoch 48/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7382 - accuracy: 0.4124 - val_loss: 1.6823 - val_accuracy: 0.4452\n",
      "Epoch 49/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7381 - accuracy: 0.4171 - val_loss: 1.6800 - val_accuracy: 0.4476\n",
      "Epoch 50/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7373 - accuracy: 0.4200 - val_loss: 1.6766 - val_accuracy: 0.4476\n",
      "Epoch 51/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7359 - accuracy: 0.4188 - val_loss: 1.6768 - val_accuracy: 0.4429\n",
      "Epoch 52/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7357 - accuracy: 0.4182 - val_loss: 1.6750 - val_accuracy: 0.4476\n",
      "Epoch 53/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7351 - accuracy: 0.4118 - val_loss: 1.6718 - val_accuracy: 0.4476\n",
      "Epoch 54/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7340 - accuracy: 0.4200 - val_loss: 1.6765 - val_accuracy: 0.4476\n",
      "Epoch 55/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7340 - accuracy: 0.4165 - val_loss: 1.6735 - val_accuracy: 0.4476\n",
      "Epoch 56/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7341 - accuracy: 0.4136 - val_loss: 1.6748 - val_accuracy: 0.4476\n",
      "Epoch 57/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7331 - accuracy: 0.4171 - val_loss: 1.6710 - val_accuracy: 0.4429\n",
      "Epoch 58/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7323 - accuracy: 0.4188 - val_loss: 1.6744 - val_accuracy: 0.4476\n",
      "Epoch 59/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7323 - accuracy: 0.4118 - val_loss: 1.6713 - val_accuracy: 0.4452\n",
      "Epoch 60/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7324 - accuracy: 0.4130 - val_loss: 1.6690 - val_accuracy: 0.4476\n",
      "Epoch 61/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7322 - accuracy: 0.4200 - val_loss: 1.6721 - val_accuracy: 0.4476\n",
      "Epoch 62/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7320 - accuracy: 0.4153 - val_loss: 1.6690 - val_accuracy: 0.4476\n",
      "Epoch 63/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7309 - accuracy: 0.4100 - val_loss: 1.6691 - val_accuracy: 0.4476\n",
      "Epoch 64/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7304 - accuracy: 0.4176 - val_loss: 1.6708 - val_accuracy: 0.4429\n",
      "Epoch 65/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7292 - accuracy: 0.4194 - val_loss: 1.6680 - val_accuracy: 0.4476\n",
      "Epoch 66/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7302 - accuracy: 0.4176 - val_loss: 1.6692 - val_accuracy: 0.4429\n",
      "Epoch 67/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7294 - accuracy: 0.4159 - val_loss: 1.6683 - val_accuracy: 0.4452\n",
      "Epoch 68/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7286 - accuracy: 0.4194 - val_loss: 1.6682 - val_accuracy: 0.4476\n",
      "Epoch 69/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7285 - accuracy: 0.4159 - val_loss: 1.6675 - val_accuracy: 0.4429\n",
      "Epoch 70/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7278 - accuracy: 0.4171 - val_loss: 1.6671 - val_accuracy: 0.4429\n",
      "Epoch 71/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7274 - accuracy: 0.4153 - val_loss: 1.6681 - val_accuracy: 0.4476\n",
      "Epoch 72/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7264 - accuracy: 0.4200 - val_loss: 1.6690 - val_accuracy: 0.4476\n",
      "Epoch 73/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7272 - accuracy: 0.4200 - val_loss: 1.6669 - val_accuracy: 0.4476\n",
      "Epoch 74/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7259 - accuracy: 0.4171 - val_loss: 1.6668 - val_accuracy: 0.4429\n",
      "Epoch 75/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7282 - accuracy: 0.4147 - val_loss: 1.6667 - val_accuracy: 0.4476\n",
      "Epoch 76/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7258 - accuracy: 0.4200 - val_loss: 1.6677 - val_accuracy: 0.4429\n",
      "Epoch 77/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7276 - accuracy: 0.4194 - val_loss: 1.6639 - val_accuracy: 0.4476\n",
      "Epoch 78/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7251 - accuracy: 0.4136 - val_loss: 1.6608 - val_accuracy: 0.4429\n",
      "Epoch 79/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7262 - accuracy: 0.4153 - val_loss: 1.6637 - val_accuracy: 0.4476\n",
      "Epoch 80/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7260 - accuracy: 0.4200 - val_loss: 1.6669 - val_accuracy: 0.4476\n",
      "Epoch 81/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7257 - accuracy: 0.4200 - val_loss: 1.6643 - val_accuracy: 0.4476\n",
      "Epoch 82/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7240 - accuracy: 0.4200 - val_loss: 1.6658 - val_accuracy: 0.4476\n",
      "Epoch 83/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7244 - accuracy: 0.4171 - val_loss: 1.6640 - val_accuracy: 0.4476\n",
      "Epoch 84/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7259 - accuracy: 0.4182 - val_loss: 1.6636 - val_accuracy: 0.4476\n",
      "Epoch 85/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7251 - accuracy: 0.4194 - val_loss: 1.6632 - val_accuracy: 0.4429\n",
      "Epoch 86/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7238 - accuracy: 0.4136 - val_loss: 1.6633 - val_accuracy: 0.4476\n",
      "Epoch 87/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7252 - accuracy: 0.4141 - val_loss: 1.6638 - val_accuracy: 0.4476\n",
      "Epoch 88/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7262 - accuracy: 0.4159 - val_loss: 1.6619 - val_accuracy: 0.4476\n",
      "Epoch 89/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7240 - accuracy: 0.4200 - val_loss: 1.6626 - val_accuracy: 0.4476\n",
      "Epoch 90/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7238 - accuracy: 0.4194 - val_loss: 1.6632 - val_accuracy: 0.4429\n",
      "Epoch 91/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7234 - accuracy: 0.4153 - val_loss: 1.6633 - val_accuracy: 0.4452\n",
      "Epoch 92/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7237 - accuracy: 0.4182 - val_loss: 1.6617 - val_accuracy: 0.4476\n",
      "Epoch 93/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7237 - accuracy: 0.4182 - val_loss: 1.6634 - val_accuracy: 0.4476\n",
      "Epoch 94/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7237 - accuracy: 0.4200 - val_loss: 1.6611 - val_accuracy: 0.4476\n",
      "Epoch 95/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7233 - accuracy: 0.4165 - val_loss: 1.6611 - val_accuracy: 0.4452\n",
      "Epoch 96/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7231 - accuracy: 0.4176 - val_loss: 1.6616 - val_accuracy: 0.4476\n",
      "Epoch 97/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7226 - accuracy: 0.4182 - val_loss: 1.6625 - val_accuracy: 0.4429\n",
      "Epoch 98/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7246 - accuracy: 0.4194 - val_loss: 1.6617 - val_accuracy: 0.4429\n",
      "Epoch 99/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7236 - accuracy: 0.4194 - val_loss: 1.6594 - val_accuracy: 0.4476\n",
      "Epoch 100/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7225 - accuracy: 0.4200 - val_loss: 1.6597 - val_accuracy: 0.4476\n",
      "Epoch 101/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7226 - accuracy: 0.4182 - val_loss: 1.6617 - val_accuracy: 0.4476\n",
      "Epoch 102/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7221 - accuracy: 0.4194 - val_loss: 1.6606 - val_accuracy: 0.4476\n",
      "Epoch 103/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7232 - accuracy: 0.4182 - val_loss: 1.6612 - val_accuracy: 0.4429\n",
      "Epoch 104/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7224 - accuracy: 0.4141 - val_loss: 1.6609 - val_accuracy: 0.4476\n",
      "Epoch 105/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7232 - accuracy: 0.4176 - val_loss: 1.6600 - val_accuracy: 0.4429\n",
      "Epoch 106/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7237 - accuracy: 0.4182 - val_loss: 1.6634 - val_accuracy: 0.4476\n",
      "Epoch 107/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7228 - accuracy: 0.4171 - val_loss: 1.6601 - val_accuracy: 0.4476\n",
      "Epoch 108/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7227 - accuracy: 0.4188 - val_loss: 1.6596 - val_accuracy: 0.4429\n",
      "Epoch 109/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7225 - accuracy: 0.4176 - val_loss: 1.6606 - val_accuracy: 0.4476\n",
      "Epoch 110/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7213 - accuracy: 0.4200 - val_loss: 1.6581 - val_accuracy: 0.4476\n",
      "Epoch 111/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7213 - accuracy: 0.4124 - val_loss: 1.6589 - val_accuracy: 0.4429\n",
      "Epoch 112/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7224 - accuracy: 0.4171 - val_loss: 1.6602 - val_accuracy: 0.4476\n",
      "Epoch 113/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7206 - accuracy: 0.4165 - val_loss: 1.6626 - val_accuracy: 0.4476\n",
      "Epoch 114/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7206 - accuracy: 0.4200 - val_loss: 1.6591 - val_accuracy: 0.4476\n",
      "Epoch 115/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7219 - accuracy: 0.4200 - val_loss: 1.6587 - val_accuracy: 0.4476\n",
      "Epoch 116/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7213 - accuracy: 0.4188 - val_loss: 1.6599 - val_accuracy: 0.4429\n",
      "Epoch 117/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7207 - accuracy: 0.4176 - val_loss: 1.6594 - val_accuracy: 0.4476\n",
      "Epoch 118/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7218 - accuracy: 0.4112 - val_loss: 1.6588 - val_accuracy: 0.4476\n",
      "Epoch 119/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7204 - accuracy: 0.4200 - val_loss: 1.6598 - val_accuracy: 0.4476\n",
      "Epoch 120/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7206 - accuracy: 0.4188 - val_loss: 1.6598 - val_accuracy: 0.4429\n",
      "Epoch 121/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7212 - accuracy: 0.4176 - val_loss: 1.6598 - val_accuracy: 0.4476\n",
      "Epoch 122/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7221 - accuracy: 0.4200 - val_loss: 1.6591 - val_accuracy: 0.4476\n",
      "Epoch 123/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7211 - accuracy: 0.4182 - val_loss: 1.6614 - val_accuracy: 0.4429\n",
      "Epoch 124/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7207 - accuracy: 0.4194 - val_loss: 1.6594 - val_accuracy: 0.4476\n",
      "Epoch 125/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7207 - accuracy: 0.4124 - val_loss: 1.6584 - val_accuracy: 0.4429\n",
      "Epoch 126/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7211 - accuracy: 0.4159 - val_loss: 1.6591 - val_accuracy: 0.4452\n",
      "Epoch 127/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7205 - accuracy: 0.4182 - val_loss: 1.6597 - val_accuracy: 0.4429\n",
      "Epoch 128/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7211 - accuracy: 0.4136 - val_loss: 1.6597 - val_accuracy: 0.4476\n",
      "Epoch 129/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7215 - accuracy: 0.4200 - val_loss: 1.6589 - val_accuracy: 0.4476\n",
      "Epoch 130/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7201 - accuracy: 0.4153 - val_loss: 1.6577 - val_accuracy: 0.4476\n",
      "Epoch 131/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7208 - accuracy: 0.4171 - val_loss: 1.6590 - val_accuracy: 0.4476\n",
      "Epoch 132/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7196 - accuracy: 0.4176 - val_loss: 1.6597 - val_accuracy: 0.4429\n",
      "Epoch 133/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7192 - accuracy: 0.4194 - val_loss: 1.6570 - val_accuracy: 0.4476\n",
      "Epoch 134/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7205 - accuracy: 0.4136 - val_loss: 1.6601 - val_accuracy: 0.4476\n",
      "Epoch 135/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7198 - accuracy: 0.4200 - val_loss: 1.6594 - val_accuracy: 0.4476\n",
      "Epoch 136/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7208 - accuracy: 0.4200 - val_loss: 1.6612 - val_accuracy: 0.4476\n",
      "Epoch 137/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7200 - accuracy: 0.4200 - val_loss: 1.6569 - val_accuracy: 0.4476\n",
      "Epoch 138/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7206 - accuracy: 0.4200 - val_loss: 1.6601 - val_accuracy: 0.4476\n",
      "Epoch 139/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7208 - accuracy: 0.4124 - val_loss: 1.6595 - val_accuracy: 0.4476\n",
      "Epoch 140/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7203 - accuracy: 0.4182 - val_loss: 1.6554 - val_accuracy: 0.4429\n",
      "Epoch 141/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7201 - accuracy: 0.4165 - val_loss: 1.6582 - val_accuracy: 0.4476\n",
      "Epoch 142/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7204 - accuracy: 0.4153 - val_loss: 1.6558 - val_accuracy: 0.4452\n",
      "Epoch 143/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7240 - accuracy: 0.4211 - val_loss: 1.6608 - val_accuracy: 0.4476\n",
      "Epoch 144/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7191 - accuracy: 0.4147 - val_loss: 1.6553 - val_accuracy: 0.4476\n",
      "Epoch 145/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7202 - accuracy: 0.4159 - val_loss: 1.6594 - val_accuracy: 0.4476\n",
      "Epoch 146/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7211 - accuracy: 0.4200 - val_loss: 1.6562 - val_accuracy: 0.4476\n",
      "Epoch 147/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7200 - accuracy: 0.4147 - val_loss: 1.6593 - val_accuracy: 0.4452\n",
      "Epoch 148/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7195 - accuracy: 0.4130 - val_loss: 1.6581 - val_accuracy: 0.4429\n",
      "Epoch 149/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7199 - accuracy: 0.4200 - val_loss: 1.6574 - val_accuracy: 0.4476\n",
      "Epoch 150/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7193 - accuracy: 0.4200 - val_loss: 1.6581 - val_accuracy: 0.4476\n",
      "Epoch 151/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7200 - accuracy: 0.4200 - val_loss: 1.6561 - val_accuracy: 0.4476\n",
      "Epoch 152/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7188 - accuracy: 0.4171 - val_loss: 1.6575 - val_accuracy: 0.4429\n",
      "Epoch 153/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7187 - accuracy: 0.4188 - val_loss: 1.6579 - val_accuracy: 0.4476\n",
      "Epoch 154/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7191 - accuracy: 0.4153 - val_loss: 1.6600 - val_accuracy: 0.4429\n",
      "Epoch 155/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7212 - accuracy: 0.4136 - val_loss: 1.6565 - val_accuracy: 0.4476\n",
      "Epoch 156/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7196 - accuracy: 0.4165 - val_loss: 1.6562 - val_accuracy: 0.4476\n",
      "Epoch 157/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7197 - accuracy: 0.4124 - val_loss: 1.6579 - val_accuracy: 0.4476\n",
      "Epoch 158/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7194 - accuracy: 0.4200 - val_loss: 1.6591 - val_accuracy: 0.4476\n",
      "Epoch 159/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7191 - accuracy: 0.4171 - val_loss: 1.6592 - val_accuracy: 0.4429\n",
      "Epoch 160/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7189 - accuracy: 0.4200 - val_loss: 1.6567 - val_accuracy: 0.4476\n",
      "Epoch 161/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7188 - accuracy: 0.4200 - val_loss: 1.6560 - val_accuracy: 0.4476\n",
      "Epoch 162/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7202 - accuracy: 0.4176 - val_loss: 1.6587 - val_accuracy: 0.4429\n",
      "Epoch 163/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7177 - accuracy: 0.4188 - val_loss: 1.6588 - val_accuracy: 0.4476\n",
      "Epoch 164/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7195 - accuracy: 0.4188 - val_loss: 1.6570 - val_accuracy: 0.4429\n",
      "Epoch 165/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7196 - accuracy: 0.4194 - val_loss: 1.6594 - val_accuracy: 0.4476\n",
      "Epoch 166/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7191 - accuracy: 0.4200 - val_loss: 1.6583 - val_accuracy: 0.4476\n",
      "Epoch 167/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7189 - accuracy: 0.4106 - val_loss: 1.6561 - val_accuracy: 0.4476\n",
      "Epoch 168/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7183 - accuracy: 0.4171 - val_loss: 1.6563 - val_accuracy: 0.4476\n",
      "Epoch 169/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7186 - accuracy: 0.4188 - val_loss: 1.6566 - val_accuracy: 0.4476\n",
      "Epoch 170/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7201 - accuracy: 0.4141 - val_loss: 1.6582 - val_accuracy: 0.4406\n",
      "Epoch 171/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7200 - accuracy: 0.4188 - val_loss: 1.6551 - val_accuracy: 0.4429\n",
      "Epoch 172/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7190 - accuracy: 0.4089 - val_loss: 1.6573 - val_accuracy: 0.4406\n",
      "Epoch 173/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7218 - accuracy: 0.4153 - val_loss: 1.6583 - val_accuracy: 0.4476\n",
      "Epoch 174/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7198 - accuracy: 0.4200 - val_loss: 1.6563 - val_accuracy: 0.4476\n",
      "Epoch 175/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7179 - accuracy: 0.4136 - val_loss: 1.6572 - val_accuracy: 0.4476\n",
      "Epoch 176/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7190 - accuracy: 0.4200 - val_loss: 1.6583 - val_accuracy: 0.4476\n",
      "Epoch 177/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7186 - accuracy: 0.4176 - val_loss: 1.6559 - val_accuracy: 0.4476\n",
      "Epoch 178/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7191 - accuracy: 0.4188 - val_loss: 1.6593 - val_accuracy: 0.4476\n",
      "Epoch 179/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7191 - accuracy: 0.4165 - val_loss: 1.6542 - val_accuracy: 0.4476\n",
      "Epoch 180/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7192 - accuracy: 0.4182 - val_loss: 1.6598 - val_accuracy: 0.4476\n",
      "Epoch 181/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7191 - accuracy: 0.4200 - val_loss: 1.6563 - val_accuracy: 0.4476\n",
      "Epoch 182/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7201 - accuracy: 0.4159 - val_loss: 1.6579 - val_accuracy: 0.4476\n",
      "Epoch 183/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7185 - accuracy: 0.4165 - val_loss: 1.6567 - val_accuracy: 0.4476\n",
      "Epoch 184/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7181 - accuracy: 0.4182 - val_loss: 1.6569 - val_accuracy: 0.4476\n",
      "Epoch 185/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7190 - accuracy: 0.4141 - val_loss: 1.6578 - val_accuracy: 0.4429\n",
      "Epoch 186/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7176 - accuracy: 0.4188 - val_loss: 1.6558 - val_accuracy: 0.4476\n",
      "Epoch 187/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7185 - accuracy: 0.4147 - val_loss: 1.6575 - val_accuracy: 0.4476\n",
      "Epoch 188/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7194 - accuracy: 0.4171 - val_loss: 1.6573 - val_accuracy: 0.4476\n",
      "Epoch 189/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7186 - accuracy: 0.4153 - val_loss: 1.6546 - val_accuracy: 0.4476\n",
      "Epoch 190/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7184 - accuracy: 0.4176 - val_loss: 1.6560 - val_accuracy: 0.4476\n",
      "Epoch 191/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7189 - accuracy: 0.4200 - val_loss: 1.6543 - val_accuracy: 0.4476\n",
      "Epoch 192/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7188 - accuracy: 0.4106 - val_loss: 1.6616 - val_accuracy: 0.4476\n",
      "Epoch 193/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7189 - accuracy: 0.4200 - val_loss: 1.6566 - val_accuracy: 0.4476\n",
      "Epoch 194/200\n",
      "27/27 [==============================] - 0s 5ms/step - loss: 1.7179 - accuracy: 0.4176 - val_loss: 1.6578 - val_accuracy: 0.4476\n",
      "Epoch 195/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7191 - accuracy: 0.4165 - val_loss: 1.6571 - val_accuracy: 0.4476\n",
      "Epoch 196/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7193 - accuracy: 0.4159 - val_loss: 1.6551 - val_accuracy: 0.4429\n",
      "Epoch 197/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7185 - accuracy: 0.4176 - val_loss: 1.6566 - val_accuracy: 0.4476\n",
      "Epoch 198/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7181 - accuracy: 0.4200 - val_loss: 1.6578 - val_accuracy: 0.4476\n",
      "Epoch 199/200\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.7188 - accuracy: 0.4194 - val_loss: 1.6564 - val_accuracy: 0.4476\n",
      "Epoch 200/200\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.7174 - accuracy: 0.4176 - val_loss: 1.6560 - val_accuracy: 0.4429\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.6560 - accuracy: 0.4429\n",
      "Test accuracy: 0.4428904354572296\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "data = pd.read_csv('/content/new_dataset_filled.csv')\n",
    "\n",
    "# Encode product types and modules\n",
    "encoder_product_type = LabelEncoder()\n",
    "encoder_module = LabelEncoder()\n",
    "data['Product Type'] = encoder_product_type.fit_transform(data['Product Type'])\n",
    "data['Module'] = encoder_module.fit_transform(data['Module'])\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = data['Product Type'].values\n",
    "y = data['Module'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the number of unique product types and modules\n",
    "num_product_types = len(encoder_product_type.classes_)\n",
    "num_modules = len(encoder_module.classes_)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_product_types, output_dim=32, input_length=1))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(num_modules, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_accuracy}')\n",
    "\n",
    "# Save the model for future use\n",
    "model.save('module_prediction_rnn_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lu8Rc9PioD4_",
    "outputId": "7beac1f7-6cd6-4034-f673-f152b6eb2e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "27/27 [==============================] - 2s 17ms/step - loss: 3.0734 - accuracy: 0.1893 - val_loss: 3.0484 - val_accuracy: 0.2098\n",
      "Epoch 2/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 3.0111 - accuracy: 0.2091 - val_loss: 2.9490 - val_accuracy: 0.2098\n",
      "Epoch 3/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.8474 - accuracy: 0.2173 - val_loss: 2.7022 - val_accuracy: 0.2331\n",
      "Epoch 4/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.5616 - accuracy: 0.2401 - val_loss: 2.4396 - val_accuracy: 0.2774\n",
      "Epoch 5/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.3863 - accuracy: 0.2850 - val_loss: 2.3181 - val_accuracy: 0.3660\n",
      "Epoch 6/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.2892 - accuracy: 0.3376 - val_loss: 2.2258 - val_accuracy: 0.3800\n",
      "Epoch 7/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.2135 - accuracy: 0.3475 - val_loss: 2.1420 - val_accuracy: 0.3287\n",
      "Epoch 8/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.1463 - accuracy: 0.3364 - val_loss: 2.0748 - val_accuracy: 0.3800\n",
      "Epoch 9/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.0898 - accuracy: 0.3715 - val_loss: 2.0210 - val_accuracy: 0.4033\n",
      "Epoch 10/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 2.0409 - accuracy: 0.3908 - val_loss: 1.9797 - val_accuracy: 0.4126\n",
      "Epoch 11/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.9995 - accuracy: 0.3931 - val_loss: 1.9419 - val_accuracy: 0.4126\n",
      "Epoch 12/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.9614 - accuracy: 0.3931 - val_loss: 1.9096 - val_accuracy: 0.4126\n",
      "Epoch 13/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.9305 - accuracy: 0.3931 - val_loss: 1.8831 - val_accuracy: 0.4126\n",
      "Epoch 14/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.9043 - accuracy: 0.3931 - val_loss: 1.8650 - val_accuracy: 0.4126\n",
      "Epoch 15/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.8842 - accuracy: 0.3966 - val_loss: 1.8437 - val_accuracy: 0.4312\n",
      "Epoch 16/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.8677 - accuracy: 0.4001 - val_loss: 1.8305 - val_accuracy: 0.4382\n",
      "Epoch 17/20\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 1.8560 - accuracy: 0.4083 - val_loss: 1.8135 - val_accuracy: 0.4359\n",
      "Epoch 18/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.8423 - accuracy: 0.4106 - val_loss: 1.7994 - val_accuracy: 0.4312\n",
      "Epoch 19/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.8299 - accuracy: 0.4130 - val_loss: 1.7888 - val_accuracy: 0.4336\n",
      "Epoch 20/20\n",
      "27/27 [==============================] - 0s 3ms/step - loss: 1.8211 - accuracy: 0.4130 - val_loss: 1.7770 - val_accuracy: 0.4336\n",
      "1/1 [==============================] - 0s 372ms/step\n",
      "Predicted Module for Product Type 'T-shirt': AL32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "data = pd.read_csv('/content/new_dataset.csv')\n",
    "\n",
    "# Encode product types and modules\n",
    "encoder_product_type = LabelEncoder()\n",
    "encoder_module = LabelEncoder()\n",
    "data['Product Type'] = encoder_product_type.fit_transform(data['Product Type'])\n",
    "data['Module'] = encoder_module.fit_transform(data['Module'])\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = data['Product Type'].values\n",
    "y = data['Module'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the number of unique product types and modules\n",
    "num_product_types = len(encoder_product_type.classes_)\n",
    "num_modules = len(encoder_module.classes_)\n",
    "\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_product_types, output_dim=32, input_length=1))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(num_modules, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the model for future use\n",
    "model.save('module_prediction_rnn_model')\n",
    "\n",
    "# Load the trained model\n",
    "model = keras.models.load_model('module_prediction_rnn_model')\n",
    "\n",
    "# Function to predict module based on product type\n",
    "def predict_module(product_type):\n",
    "    # Encode the product type\n",
    "    encoded_product_type = encoder_product_type.transform([product_type])\n",
    "    input_data = np.array(encoded_product_type)\n",
    "    input_data = input_data.reshape((1, 1))  # Reshape for model input (batch_size, time_steps, input_dim)\n",
    "\n",
    "    # Use the model to make predictions\n",
    "    predicted_module_encoded = model.predict(input_data)\n",
    "    predicted_module = encoder_module.inverse_transform([np.argmax(predicted_module_encoded)])\n",
    "\n",
    "    return predicted_module[0]\n",
    "\n",
    "# Test the model with a product type\n",
    "product_type_to_predict = \"T-shirt\"  # Replace with the actual product type you want to predict\n",
    "predicted_module = predict_module(product_type_to_predict)\n",
    "print(f\"Predicted Module for Product Type '{product_type_to_predict}': {predicted_module}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grAgu38LvXiU",
    "outputId": "0f0177a6-c13f-478c-93c6-4678c87993f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9906759906759907\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('/content/new_dataset_filled.csv')\n",
    "\n",
    "# Replace non-numeric values (e.g., '-') with NaN\n",
    "data = data.replace('-', float('nan'))\n",
    "\n",
    "# Additional Feature Engineering\n",
    "data['Defect_Rate'] = data['Defect Qty'] / data['Total SMV']\n",
    "data['Avg_Diff'] = data['Actual Per Day'] - data['Avg Per Day']\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "data['Module'] = le.fit_transform(data['Module'])\n",
    "data['Product Type'] = le.fit_transform(data['Product Type'])\n",
    "\n",
    "# Replace missing values with the mean of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "columns_to_impute = ['Total SMV', 'Avg Per Day', 'Actual Per Day', 'Defect Qty', 'Reject Qty', 'Missing Qty', 'Defect_Rate', 'Avg_Diff']\n",
    "data[columns_to_impute] = imputer.fit_transform(data[columns_to_impute])\n",
    "\n",
    "# Define features and target variable\n",
    "X = data[['Module', 'Product Type', 'Total SMV', 'Avg Per Day', 'Actual Per Day', 'Defect Qty', 'Reject Qty', 'Missing Qty', 'Defect_Rate', 'Avg_Diff']]\n",
    "# Replace 'Your_Target_Column_Name' with the actual name of your target variable.\n",
    "y = data['Product Type']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzBI2TZtv5Y3",
    "outputId": "63578fab-d1ef-4ede-a931-0b38bf342ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Defect Type for Predicted Line: Shading between Garment/within set/Part to part\n",
      "Predicted Module: AL02\n",
      "Total SMV for Predicted Module: 10.406078431372547\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from joblib import dump\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('/content/new_dataset_filled.csv')\n",
    "\n",
    "# Replace non-numeric values (e.g., '-') with NaN\n",
    "data = data.replace('-', float('nan'))\n",
    "\n",
    "# Additional Feature Engineering\n",
    "data['Defect_Rate'] = data['Defect Qty'] / data['Total SMV']\n",
    "data['Avg_Diff'] = data['Actual Per Day'] - data['Avg Per Day']\n",
    "\n",
    "# Encode categorical variables\n",
    "le_product = LabelEncoder()\n",
    "le_module = LabelEncoder()\n",
    "\n",
    "data['Product Type'] = le_product.fit_transform(data['Product Type'])\n",
    "data['Module'] = le_module.fit_transform(data['Module'])\n",
    "\n",
    "# Replace missing values with the mean of the column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "columns_to_impute = ['Total SMV', 'Avg Per Day', 'Actual Per Day', 'Defect Qty', 'Reject Qty', 'Missing Qty', 'Defect_Rate', 'Avg_Diff']\n",
    "data[columns_to_impute] = imputer.fit_transform(data[columns_to_impute])\n",
    "\n",
    "# Define features and target variable\n",
    "X = data[['Product Type']]\n",
    "y = data['Module']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Input the Product Type to predict the Module\n",
    "product_type_input = le_product.transform(['Hoody Baselayer'])[0]\n",
    "\n",
    "# Create a sample data point to predict\n",
    "sample_data = pd.DataFrame({'Product Type': [product_type_input]})\n",
    "\n",
    "# ... (previous code)\n",
    "\n",
    "# Predict the module\n",
    "predicted_module = clf.predict(sample_data)\n",
    "predicted_module = le_module.inverse_transform(predicted_module)\n",
    "predicted_module_name = predicted_module[0]\n",
    "\n",
    "# Find Total SMV for the predicted module\n",
    "total_smv_for_module = data[data['Module'] == product_type_input]['Total SMV'].mean()\n",
    "\n",
    "# Filter the dataset to include only rows with the predicted module\n",
    "predicted_module_data = data[data['Module'] == product_type_input]\n",
    "\n",
    "# Check if there are any records for the predicted module\n",
    "if not predicted_module_data.empty:\n",
    "    # Check if there are any Defect Names available for the predicted module\n",
    "    if 'Defect Name' in predicted_module_data.columns:\n",
    "        # Use value_counts to count the occurrences of each Defect Name\n",
    "        defect_name_counts = predicted_module_data['Defect Name'].value_counts()\n",
    "\n",
    "        if not defect_name_counts.empty:\n",
    "            # Get the most common Defect Name\n",
    "            most_common_defect_name = defect_name_counts.idxmax()\n",
    "            print(f'Most Common Defect Type for Predicted Line: {most_common_defect_name}')\n",
    "        else:\n",
    "            print('No Defect Type available for the predicted module.')\n",
    "    else:\n",
    "        print('No \"Defect Type\" column available for the predicted module.')\n",
    "else:\n",
    "    print('No records found for the predicted line.')\n",
    "\n",
    "print(f'Predicted Module: {predicted_module_name}')\n",
    "print(f'Total SMV for Predicted Module: {total_smv_for_module}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtNPLCro3R0J",
    "outputId": "f69a99c1-91d3-4085-b5f8-3946c43350f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Module: AL25\n",
      "Predicted Module: nan\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "\n",
    "# Load the trained model\n",
    "model_filename = '/content/random_forest_model.joblib'\n",
    "clf = load(model_filename)\n",
    "\n",
    "# Load the LabelEncoder used during training for 'Product Type'\n",
    "le_product = load('/content/product_type_label_encoder.joblib')  # Replace with the actual filename of your LabelEncoder\n",
    "\n",
    "# Define a function to make predictions\n",
    "def predict_module(product_type):\n",
    "    # Encode the 'Product Type' using the LabelEncoder\n",
    "    product_type_encoded = le_product.transform([product_type])[0]\n",
    "\n",
    "    # Create a sample data point to predict\n",
    "    sample_data = pd.DataFrame({'Product Type': [product_type_encoded]})\n",
    "\n",
    "    # Predict the module\n",
    "    predicted_module = clf.predict(sample_data)\n",
    "\n",
    "    # Inverse transform to get the module name\n",
    "    predicted_module = le_module.inverse_transform(predicted_module)\n",
    "\n",
    "    return predicted_module[0]\n",
    "\n",
    "# Example usage:\n",
    "product_type_to_predict = 'T-shirt Baselayer'  # Replace with the product type you want to predict\n",
    "predicted_module = predict_module(product_type_to_predict)\n",
    "print(f'Predicted Module: {predicted_module}')\n",
    "print(f'Predicted Module: {total_smv_for_module}')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
